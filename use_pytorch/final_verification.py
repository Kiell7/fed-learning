import sys
sys.path.append('..')

from get_dataset import GetDataset
from utils.nlp_transform import NLPTransform

def char_to_token_accurate(text, answer_text, char_start, nlp):
    """ç²¾ç¡®çš„å­—ç¬¦åˆ°tokenä½ç½®æ˜ å°„"""
    tokens = nlp.basic_tokenizer(text)
    answer_tokens = nlp.basic_tokenizer(answer_text)
    
    # æ–¹æ³•1: åœ¨tokensä¸­ç›´æŽ¥æŸ¥æ‰¾ç­”æ¡ˆtokenåºåˆ—
    for i in range(len(tokens) - len(answer_tokens) + 1):
        if tokens[i:i+len(answer_tokens)] == answer_tokens:
            return i, i + len(answer_tokens) - 1, tokens, True
    
    # æ–¹æ³•2: å­—ç¬¦ä½ç½®æ˜ å°„ï¼ˆä½œä¸ºåŽå¤‡ï¼‰
    text_lower = text.lower()
    char_pos = 0
    token_char_map = []
    
    for idx, token in enumerate(tokens):
        token_start = text_lower.find(token, char_pos)
        if token_start >= 0:
            token_end = token_start + len(token)
            token_char_map.append((idx, token_start, token_end))
            char_pos = token_end
        else:
            token_char_map.append((idx, char_pos, char_pos))
    
    answer_char_end = char_start + len(answer_text)
    start_token = 0
    end_token = 0
    
    for idx, ts, te in token_char_map:
        if ts <= char_start < te:
            start_token = idx
        if ts < answer_char_end <= te:
            end_token = idx
            break
    
    if end_token == 0 or end_token < start_token:
        end_token = min(start_token + len(answer_tokens) - 1, len(tokens) - 1)
    
    return start_token, end_token, tokens, False

# åŠ è½½æ•°æ®
print("åŠ è½½ SQuAD æ•°æ®é›†...")
dataset = GetDataset('squad', '../data')
contexts, questions, answers = dataset.get_raw_data('train')

# æž„å»ºè¯æ±‡è¡¨
print("æž„å»ºè¯æ±‡è¡¨...")
nlp = NLPTransform(max_length=512)
all_texts = contexts[:5000] + questions[:5000]
nlp.build_vocab(all_texts, max_vocab_size=50000)

print("\n" + "="*80)
print(f"åœ¨ 1000 ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œæœ€ç»ˆéªŒè¯")
print("="*80)

method1_success = 0  # ç›´æŽ¥åŒ¹é…æˆåŠŸ
method2_success = 0  # å­—ç¬¦æ˜ å°„æˆåŠŸ
total = 0
failed_cases = []

for i in range(1000):
    context = contexts[i]
    answer = answers[i]
    
    char_start = answer['start']
    answer_text = answer['text']
    
    start_pos, end_pos, tokens, direct_match = char_to_token_accurate(
        context, answer_text, char_start, nlp
    )
    
    answer_tokens = nlp.basic_tokenizer(answer_text)
    
    if start_pos < len(tokens) and end_pos < len(tokens):
        pred_tokens = tokens[start_pos:end_pos+1]
        
        if pred_tokens == answer_tokens:
            if direct_match:
                method1_success += 1
            else:
                method2_success += 1
        else:
            failed_cases.append({
                'idx': i,
                'answer': answer_text,
                'answer_tokens': answer_tokens,
                'pred_tokens': pred_tokens,
                'context_snippet': context[max(0, char_start-50):char_start+len(answer_text)+50]
            })
    
    total += 1

print(f"\nç»“æžœ:")
print(f"  æ€»æ ·æœ¬æ•°: {total}")
print(f"  æ–¹æ³•1æˆåŠŸ (ç›´æŽ¥tokenåŒ¹é…): {method1_success} ({100*method1_success/total:.1f}%)")
print(f"  æ–¹æ³•2æˆåŠŸ (å­—ç¬¦ä½ç½®æ˜ å°„): {method2_success} ({100*method2_success/total:.1f}%)")
print(f"  æ€»æˆåŠŸçŽ‡: {method1_success + method2_success}/{total} ({100*(method1_success+method2_success)/total:.1f}%)")
print(f"  å¤±è´¥: {len(failed_cases)} ({100*len(failed_cases)/total:.1f}%)")

if failed_cases:
    print(f"\nå‰5ä¸ªå¤±è´¥æ¡ˆä¾‹:")
    for i, case in enumerate(failed_cases[:5]):
        print(f"\n{i+1}. æ ·æœ¬ {case['idx']}:")
        print(f"   ç­”æ¡ˆ: '{case['answer']}'")
        print(f"   ç­”æ¡ˆtokens: {case['answer_tokens']}")
        print(f"   é¢„æµ‹tokens: {case['pred_tokens']}")
        print(f"   ä¸Šä¸‹æ–‡: ...{case['context_snippet']}...")
else:
    print("\nðŸŽ‰ æ‰€æœ‰æ ·æœ¬éƒ½æˆåŠŸåŒ¹é…ï¼")