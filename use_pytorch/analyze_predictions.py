import sys
sys.path.append('..')

import torch
import numpy as np
from clients import ClientsGroup
from Models import LSTM_QA

print("=" * 80)
print("åˆ†ææ¨¡å‹é¢„æµ‹è¡Œä¸º")
print("=" * 80)

dev = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åŠ è½½æ•°æ®å’Œå®¢æˆ·ç«¯ç»„
print("åŠ è½½æ•°æ®é›†...")
clients_group = ClientsGroup(
    dataSetName='squad',
    isIID=True,
    numOfClients=100,
    dev=dev
)

# âœ… ä»ä»»æ„å®¢æˆ·ç«¯çš„æ•°æ®é›†ä¸­è·å– nlp_transform
client0 = clients_group.clients_set['client0']
nlp_transform = client0.train_ds.nlp_transform
vocab_size = nlp_transform.vocab_size

print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}")

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
checkpoint_path = "./checkpoints/lstm_squad_nc100_comm100_20251106_212801/final_model.pth"
print(f"åŠ è½½æ¨¡å‹: {checkpoint_path}")

# âœ… å…ˆåŠ è½½checkpointæŸ¥çœ‹å‚æ•°
checkpoint = torch.load(checkpoint_path)

# æ£€æŸ¥checkpointçš„ç»“æ„
if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
    print(f"âœ… æ£€æµ‹åˆ°å®Œæ•´çš„checkpointï¼ŒåŒ…å«è½®æ¬¡={checkpoint.get('round')}, å‡†ç¡®ç‡={checkpoint.get('accuracy'):.2f}%, loss={checkpoint.get('loss'):.4f}")
    
    # âœ… ä»checkpointä¸­æ¨æ–­æ¨¡å‹å‚æ•°
    embedding_weight = checkpoint['model_state_dict']['embedding.weight']
    embedding_dim = embedding_weight.shape[1]  # å®é™…æ˜¯100ï¼Œä¸æ˜¯128
    print(f"âœ… ä»checkpointæ¨æ–­: embedding_dim={embedding_dim}")
    
    # ç”¨æ­£ç¡®çš„å‚æ•°åˆ›å»ºæ¨¡å‹
    net = LSTM_QA(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=256)
    net.load_state_dict(checkpoint['model_state_dict'])
else:
    # å¦‚æœæ˜¯ç›´æ¥ä¿å­˜çš„state_dictï¼Œå°è¯•æ¨æ–­å‚æ•°
    embedding_weight = checkpoint['embedding.weight']
    embedding_dim = embedding_weight.shape[1]
    print(f"âœ… ä»state_dictæ¨æ–­: embedding_dim={embedding_dim}")
    
    net = LSTM_QA(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=256)
    net.load_state_dict(checkpoint)

net = net.to(dev)
net.eval()

print("\n1. åˆ†ææ¨¡å‹é¢„æµ‹åˆ†å¸ƒ")
print("-" * 80)

all_pred_starts = []
all_pred_ends = []
all_true_starts = []
all_true_ends = []

with torch.no_grad():
    batch_count = 0
    for batch_data in clients_group.test_data_loader:
        context, question, start_pos, end_pos = batch_data
        context = context.to(dev)
        question = question.to(dev)
        
        start_logits, end_logits = net(context, question)
        
        pred_start = torch.argmax(start_logits, dim=1).cpu().numpy()
        pred_end = torch.argmax(end_logits, dim=1).cpu().numpy()
        
        all_pred_starts.extend(pred_start.tolist())
        all_pred_ends.extend(pred_end.tolist())
        all_true_starts.extend(start_pos.numpy().tolist())
        all_true_ends.extend(end_pos.numpy().tolist())
        
        batch_count += 1
        if batch_count >= 10:  # åªçœ‹å‰1280ä¸ªæ ·æœ¬
            break

all_pred_starts = np.array(all_pred_starts)
all_pred_ends = np.array(all_pred_ends)
all_true_starts = np.array(all_true_starts)
all_true_ends = np.array(all_true_ends)

print(f"\næ ·æœ¬æ€»æ•°: {len(all_pred_starts)}")

print(f"\né¢„æµ‹çš„ start_pos åˆ†å¸ƒ:")
print(f"  æœ€å°å€¼: {all_pred_starts.min()}")
print(f"  æœ€å¤§å€¼: {all_pred_starts.max()}")
print(f"  å¹³å‡å€¼: {all_pred_starts.mean():.1f}")
print(f"  ä¸­ä½æ•°: {np.median(all_pred_starts):.1f}")
print(f"  æ ‡å‡†å·®: {all_pred_starts.std():.1f}")

print(f"\nçœŸå®çš„ start_pos åˆ†å¸ƒ:")
print(f"  æœ€å°å€¼: {all_true_starts.min()}")
print(f"  æœ€å¤§å€¼: {all_true_starts.max()}")
print(f"  å¹³å‡å€¼: {all_true_starts.mean():.1f}")
print(f"  ä¸­ä½æ•°: {np.median(all_true_starts):.1f}")
print(f"  æ ‡å‡†å·®: {all_true_starts.std():.1f}")

print(f"\né¢„æµ‹çš„ç­”æ¡ˆé•¿åº¦:")
pred_lengths = all_pred_ends - all_pred_starts + 1
print(f"  å¹³å‡é•¿åº¦: {pred_lengths.mean():.1f}")
print(f"  ä¸­ä½æ•°: {np.median(pred_lengths):.1f}")
print(f"  æœ€å¤§é•¿åº¦: {pred_lengths.max()}")
print(f"  æœ€å°é•¿åº¦: {pred_lengths.min()}")

print(f"\nçœŸå®çš„ç­”æ¡ˆé•¿åº¦:")
true_lengths = all_true_ends - all_true_starts + 1
print(f"  å¹³å‡é•¿åº¦: {true_lengths.mean():.1f}")
print(f"  ä¸­ä½æ•°: {np.median(true_lengths):.1f}")

# æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é¢„æµ‹éƒ½é›†ä¸­åœ¨æŸäº›ä½ç½®
from collections import Counter
start_counter = Counter(all_pred_starts)
most_common_starts = start_counter.most_common(10)

print(f"\nâš ï¸ æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ€»é¢„æµ‹ç›¸åŒä½ç½®:")
print(f"å‡ºç°æœ€é¢‘ç¹çš„å‰10ä¸ª start_pos:")
for pos, count in most_common_starts:
    percentage = 100 * count / len(all_pred_starts)
    print(f"  ä½ç½® {pos}: {count}æ¬¡ ({percentage:.1f}%)")

# æ£€æŸ¥ end_pos
end_counter = Counter(all_pred_ends)
most_common_ends = end_counter.most_common(10)

print(f"\nå‡ºç°æœ€é¢‘ç¹çš„å‰10ä¸ª end_pos:")
for pos, count in most_common_ends:
    percentage = 100 * count / len(all_pred_ends)
    print(f"  ä½ç½® {pos}: {count}æ¬¡ ({percentage:.1f}%)")

if most_common_starts[0][1] > len(all_pred_starts) * 0.5:
    print(f"\nâŒâŒâŒ ä¸¥é‡é—®é¢˜ï¼šè¶…è¿‡50%çš„é¢„æµ‹éƒ½æ˜¯ä½ç½® {most_common_starts[0][0]}ï¼")
    print(f"   æ¨¡å‹æ²¡æœ‰çœŸæ­£å­¦ä¹ ï¼Œåªæ˜¯è®°ä½äº†ä¸€ä¸ªå›ºå®šä½ç½®ã€‚")
elif most_common_starts[0][1] > len(all_pred_starts) * 0.2:
    print(f"\nâš ï¸ é—®é¢˜ï¼šè¶…è¿‡20%çš„é¢„æµ‹éƒ½æ˜¯ä½ç½® {most_common_starts[0][0]}")
    print(f"   æ¨¡å‹çš„å¤šæ ·æ€§ä¸è¶³ã€‚")
else:
    print(f"\nâœ… é¢„æµ‹åˆ†å¸ƒè¾ƒä¸ºåˆç†")

# åˆ†æå‡ ä¸ªå…·ä½“æ ·æœ¬
print("\n" + "=" * 80)
print("2. åˆ†æå…·ä½“é¢„æµ‹æ ·ä¾‹")
print("-" * 80)

for i in range(min(5, len(client0.train_ds))):
    sample = client0.train_ds[i]
    raw_data = client0.train_ds.data[i]
    
    if isinstance(raw_data, tuple) and len(raw_data) == 3:
        context_text, question_text, answer_dict = raw_data
        
        context = sample['context'].unsqueeze(0).to(dev)
        question = sample['question'].unsqueeze(0).to(dev)
        true_start = sample['start_pos'].item()
        true_end = sample['end_pos'].item()
        
        with torch.no_grad():
            start_logits, end_logits = net(context, question)
            pred_start = torch.argmax(start_logits, dim=1).item()
            pred_end = torch.argmax(end_logits, dim=1).item()
            
            # è·å–å‰3ä¸ªæœ€å¯èƒ½çš„ä½ç½®
            top3_starts = torch.topk(start_logits[0], 3)
            top3_ends = torch.topk(end_logits[0], 3)
        
        # è·å–tokens
        tokens = nlp_transform.basic_tokenizer(context_text)
        
        print(f"\n{'='*60}")
        print(f"æ ·æœ¬ {i}:")
        print(f"  é—®é¢˜: {question_text[:80]}...")
        print(f"  çœŸå®ç­”æ¡ˆ: '{answer_dict['text']}'")
        
        if pred_start < len(tokens) and pred_end < len(tokens) and pred_end >= pred_start:
            pred_answer = ' '.join(tokens[pred_start:pred_end+1])
            print(f"  é¢„æµ‹ç­”æ¡ˆ: '{pred_answer}'")
        else:
            print(f"  é¢„æµ‹ç­”æ¡ˆ: [æ— æ•ˆä½ç½® {pred_start}-{pred_end}]")
        
        print(f"  çœŸå®ä½ç½®: {true_start}-{true_end} (é•¿åº¦={true_end-true_start+1})")
        print(f"  é¢„æµ‹ä½ç½®: {pred_start}-{pred_end} (é•¿åº¦={pred_end-pred_start+1})")
        print(f"  ç²¾ç¡®åŒ¹é…: {'âœ…' if pred_start == true_start and pred_end == true_end else 'âŒ'}")
        
        # æ˜¾ç¤ºtop3é¢„æµ‹
        print(f"  Top3 starté¢„æµ‹:")
        for idx, (score, pos) in enumerate(zip(top3_starts.values, top3_starts.indices)):
            print(f"    {idx+1}. ä½ç½® {pos.item()}: {score.item():.3f}")
        
        print(f"  Top3 endé¢„æµ‹:")
        for idx, (score, pos) in enumerate(zip(top3_ends.values, top3_ends.indices)):
            print(f"    {idx+1}. ä½ç½® {pos.item()}: {score.item():.3f}")

print("\n" + "=" * 80)
print("3. è¯Šæ–­ç»“è®º")
print("=" * 80)

if most_common_starts[0][1] > len(all_pred_starts) * 0.3:
    print("\nâŒ ä¸»è¦é—®é¢˜ï¼šæ¨¡å‹é™·å…¥äº†ã€Œå±€éƒ¨æœ€ä¼˜ã€")
    print("   - æ¨¡å‹å­¦ä¼šäº†æ€»æ˜¯é¢„æµ‹æŸäº›å›ºå®šä½ç½®æ¥é™ä½loss")
    print("   - è¿™ä¸æ˜¯çœŸæ­£çš„å­¦ä¹ ï¼Œåªæ˜¯æ‰¾åˆ°äº†losså‡½æ•°çš„æ¼æ´")
    print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
    print("   1. å¢åŠ å®¢æˆ·ç«¯é‡‡æ ·æ¯”ä¾‹: -cf 0.3 æˆ– 0.5")
    print("   2. å‡å°‘å®¢æˆ·ç«¯æ•°é‡: -nc 50 æˆ– 20")
    print("   3. å¢åŠ å­¦ä¹ ç‡: -lr 0.002 æˆ– 0.003")
    print("   4. æ·»åŠ  Dropout é˜²æ­¢è¿‡æ‹Ÿåˆå›ºå®šæ¨¡å¼")
    print("   5. âš ï¸ é‡æ–°è®­ç»ƒï¼ˆå½“å‰æ¨¡å‹å·²ç»å­¦åäº†ï¼‰")
else:
    print("\nâœ… é¢„æµ‹åˆ†å¸ƒæ­£å¸¸")
    print("   - é—®é¢˜å¯èƒ½æ˜¯æ•°æ®ä¸è¶³æˆ–è®­ç»ƒä¸å……åˆ†")
    print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
    print("   1. å¤§å¹…å¢åŠ å®¢æˆ·ç«¯é‡‡æ ·: -cf 0.5")
    print("   2. å»¶é•¿è®­ç»ƒ: -ncomm 200")

print("\n" + "=" * 80)
print("4. æ¨èçš„è®­ç»ƒå‘½ä»¤")
print("=" * 80)
print("\næ–¹æ¡ˆ1ï¼ˆå¹³è¡¡ï¼‰ï¼š")
print("python server.py -mn lstm -vs 50000 -dsn squad -nc 50 -cf 0.3 -E 5 -B 64 -lr 0.002 -ncomm 100 -sf 5")
print("\næ–¹æ¡ˆ2ï¼ˆæ¿€è¿›ï¼Œæ¨èï¼‰ï¼š")
print("python server.py -mn lstm -vs 50000 -dsn squad -nc 20 -cf 0.5 -E 3 -B 64 -lr 0.003 -ncomm 150 -sf 10")
print("\næ–¹æ¡ˆ3ï¼ˆæé™ï¼Œæœ€å¿«æ”¶æ•›ï¼‰ï¼š")
print("python server.py -mn lstm -vs 50000 -dsn squad -nc 10 -cf 1.0 -E 2 -B 64 -lr 0.003 -ncomm 100 -sf 5")